{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77bdf63-da77-4ed3-aa83-686fd821581a",
   "metadata": {},
   "source": [
    "\n",
    "The provided code implements a stacking ensemble learning technique to combine the predictions of three base classifiers: SimpleNN (a neural network), XGBoost (an extreme gradient boosting algorithm), and SVM (support vector machine).\n",
    "\n",
    "First, the dataset is loaded and preprocessed, including standardization of features and splitting into training, validation, and test sets. Next, three base classifiers are trained on the training data: a SimpleNN model boosted with AdaBoost, an XGBoost model tuned via grid search, and an SVM model also tuned via grid search.\n",
    "\n",
    "The stacking classifier is then created using the StackingClassifier class from scikit-learn. This meta-estimator combines the predictions of the base classifiers using logistic regression as the final estimator. The class weights are adjusted to handle class imbalance.\n",
    "\n",
    "After fitting the stacking classifier on the training data, predictions are made on the validation and test sets. Performance metrics such as accuracy, precision, and recall are computed for both the validation and test sets to evaluate the stacking model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36876089-5850-46a7-b692-1c713279e0be",
   "metadata": {},
   "source": [
    "Bagging, boosting, and stacking are all ensemble learning techniques used to enhance the predictive power of machine learning models by combining the insights from multiple base models. Bagging, or Bootstrap Aggregating, involves training several independent base models on different subsets of the training data, typically through bootstrapping, and then aggregating their predictions to reduce variance and prevent overfitting. Boosting, on the other hand, builds a sequence of weak learners, each focusing on correcting the errors made by the previous models, by iteratively adjusting the weights of misclassified data points. Stacking takes a different approach by training diverse base models on the full dataset and then combining their predictions using a meta-learner, often a simple linear model, to produce the final prediction. Stacking leverages the complementary strengths of different models to achieve better performance than any individual model. While bagging creates diverse models, boosting refines weak learners, and stacking focuses on combining diverse model predictions optimally for improved performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b0e9fa-b397-47a0-84d8-303c39e4f557",
   "metadata": {},
   "source": [
    "SimpleNN:\n",
    "Accuracy: 0.6667\n",
    "Precision: 0.5000\n",
    "Recall: 0.7500\n",
    "\n",
    "XGBoost:\n",
    "Accuracy: 0.7500\n",
    "T Precision: 1.0000\n",
    "Test Recall: 0.2500\n",
    "\n",
    "SVC: \n",
    "Accuracy: 0.7500\n",
    "Precision: 1.0000\n",
    "Recall: 0.2500\n",
    "\n",
    "Stacking aggregation of the above models: ( Logistic Regression as the final estimator )\n",
    "Stacking Test Accuracy: 0.7500\n",
    "Stacking Test Precision: 1.0000\n",
    "Stacking Test Recall: 0.2500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aad96c-c05f-486f-8a1f-f22c9c3ccbdc",
   "metadata": {},
   "source": [
    "When comparing the original models to the stacking aggregated model, we can observe that the stacking model achieved similar performance in terms of accuracy and precision as the original XGBoost and SVC models. All three models, including the stacking model, achieved an accuracy of 0.7500, indicating that they correctly classified 75% of the samples in the test set. Additionally, the precision of the stacking model matches that of the original SVC model, with both achieving a precision score of 1.0000, implying that all positive predictions made by these models were indeed correct. However, the recall score of the stacking model is consistent with the recall scores of the original XGBoost and SVC models, with all three models achieving a recall of 0.2500, indicating that they correctly identified 25% of the actual positive cases in the dataset. Overall, the stacking aggregated model demonstrates comparable performance to the original models while leveraging the strengths of multiple base models through ensemble learning techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20eae9e-56e1-488d-a52d-49f82fe8f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../heart_failure_clinical_records_dataset.csv')\n",
    "\n",
    "# Rename variables for clarity\n",
    "target = 'DEATH_EVENT'\n",
    "features = df.columns[df.columns != target]\n",
    "\n",
    "# List of columns to be standardized\n",
    "columns_to_standardize = df.columns.difference(['anemia', 'diabetes', 'high_blood_pressure', 'sex', 'smoking', 'DEATH_EVENT'])\n",
    "\n",
    "# Instantiate StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.1, random_state=42)\n",
    "\n",
    "# Extract features and target variable\n",
    "X_train = df_train.drop(target, axis=1).values\n",
    "y_train = df_train[target].values\n",
    "\n",
    "# Apply SMOTE to balance the class distribution\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "X_val = df_val.drop(target, axis=1).values\n",
    "y_val = df_val[target].values\n",
    "\n",
    "X_test = df_test.drop(target, axis=1).values\n",
    "y_test = df_test[target].values\n",
    "\n",
    "# Convert data to PyTorch tensors (if needed)\n",
    "# X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "# y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Simple Neural Network (SimpleNN)\n",
    "simpleNN = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\n",
    "\n",
    "# Decision Tree as a base estimator for AdaBoost\n",
    "base_estimator = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "\n",
    "# AdaBoost for Simple Neural Network\n",
    "adaboost_simpleNN = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, random_state=42, algorithm='SAMME')\n",
    "adaboost_simpleNN.fit(X_train, y_train)\n",
    "\n",
    "# XGBoost\n",
    "param_grid_xgb = {\n",
    "    'learning_rate': [.7,.8,0.90, 0.99, 0.9999, 0.999999],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'n_estimators': [114, 116],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "xgb_classifier = XGBClassifier()\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_classifier, param_grid=param_grid_xgb, cv=10, scoring='accuracy')\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "best_xgb_classifier = grid_search_xgb.best_estimator_\n",
    "\n",
    "# SVM\n",
    "param_grid_svm = {'C': [3, 4, 5], 'gamma': [0.1, 0.2, 0.25, 0.001], 'kernel': ['linear', 'poly', 'sigmoid']}\n",
    "svm_classifier = SVC(random_state=42)\n",
    "grid_search_svm = GridSearchCV(estimator=svm_classifier, param_grid=param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_search_svm.fit(X_train, y_train)\n",
    "best_svm_classifier = grid_search_svm.best_estimator_\n",
    "\n",
    "# Stacking Classifier\n",
    "class_weights = {0: 1, 1: 3}  # Adjust weights as needed\n",
    "\n",
    "# Stacking Classifier with Logistic Regression as the final estimator and class weights\n",
    "stacking_classifier = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('simpleNN', adaboost_simpleNN),\n",
    "        ('xgb', best_xgb_classifier),\n",
    "        ('svm', best_svm_classifier)\n",
    "    ],\n",
    "    final_estimator=LogisticRegression(class_weight=class_weights),  # Use Logistic Regression and specify class weights\n",
    "    stack_method='auto',\n",
    "    cv=15\n",
    ")\n",
    "\n",
    "# Fit the stacking classifier\n",
    "stacking_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predictions on validation set\n",
    "val_predictions_stacking = stacking_classifier.predict(X_val)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_accuracy_stacking = accuracy_score(y_val, val_predictions_stacking)\n",
    "val_precision_stacking = precision_score(y_val, val_predictions_stacking)\n",
    "val_recall_stacking = recall_score(y_val, val_predictions_stacking)\n",
    "\n",
    "print(f'Stacking Validation Accuracy: {val_accuracy_stacking:.4f}')\n",
    "print(f'Stacking Validation Precision: {val_precision_stacking:.4f}')\n",
    "print(f'Stacking Validation Recall: {val_recall_stacking:.4f}')\n",
    "\n",
    "# Predictions on test set\n",
    "test_predictions_stacking = stacking_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_accuracy_stacking = accuracy_score(y_test, test_predictions_stacking)\n",
    "test_precision_stacking = precision_score(y_test, test_predictions_stacking)\n",
    "test_recall_stacking = recall_score(y_test, test_predictions_stacking)\n",
    "\n",
    "print(f'Stacking Test Accuracy: {test_accuracy_stacking:.4f}')\n",
    "print(f'Stacking Test Precision: {test_precision_stacking:.4f}')\n",
    "print(f'Stacking Test Recall: {test_recall_stacking:.4f}')\n",
    "\n",
    "# Create confusion matrix for Stacking on the test set\n",
    "conf_matrix_stacking = confusion_matrix(y_test, test_predictions_stacking)\n",
    "total_samples = len(y_test)\n",
    "sns.heatmap(conf_matrix_stacking / total_samples * 100, annot=True, fmt='.2f', cmap='Blues', cbar=False,\n",
    "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
    "            yticklabels=['Actual 0', 'Actual 1'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix (Percentage) - Stacking')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30118925-cfde-498b-9722-7d41b8e8e11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "your_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
